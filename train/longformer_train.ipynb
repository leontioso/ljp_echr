{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-11 15:09:14.467552: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-04-11 15:09:14.467595: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-04-11 15:09:14.469919: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-04-11 15:09:14.712384: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-11 15:09:16.022945: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "from transformers import TrainingArguments, Trainer, RobertaTokenizer, LongformerForSequenceClassification\n",
    "import torch\n",
    "from transformers import DataCollatorWithPadding\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from evaluate import evaluator\n",
    "import torch.cuda\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "echr = load_dataset(\"ecthr_cases\",  \"violation-prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(examples):\n",
    "    return tokenizer(examples[\"text\"],\n",
    "                     truncation=True,\n",
    "                     padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b520373ca8694b68a111ba7ca872487f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f35cb62feec046579eb1770d2d13e880",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "416c770b927a45a48b89f5a1a904450d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0701043b6af545a1b9a6b60cc6f443e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fef59409d4a477783565da498cf3ac3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee0a22fdff864225b254bf6fab01f646",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset, val_dataset, test_dataset = echr['train'], echr['validation'], echr['test']\n",
    "train_dataset, val_dataset, test_dataset = [dataset.map( lambda examples: {\"text\": \"\\n\".join(examples[\"facts\"])}) for dataset in [train_dataset, val_dataset, test_dataset]]\n",
    "train_dataset, val_dataset, test_dataset = [dataset.map(encode, batched=True) for dataset in [train_dataset, val_dataset, test_dataset]]\n",
    "train_dataset, val_dataset, test_dataset = [dataset.map( lambda examples: {'labels' :list(1 if examples['labels'][i] else 0 for i in range(len(examples['labels'])))}, batched=True) for dataset in [train_dataset, val_dataset, test_dataset]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df, val_df = pd.DataFrame(train_dataset), pd.DataFrame(test_dataset), pd.DataFrame(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df2 = Dataset.from_pandas(pd.concat([train_df[train_df['labels'] == 1].sample(762, random_state=42), train_df[train_df['labels'] == 0]]).sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = evaluate.load(\"accuracy\")\n",
    "f1 = evaluate.load(\"f1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return f1.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leontioso/anaconda3/envs/msc_thesis/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "id2label = {0: \"NON_VIOLATED\", 1: \"VIOLATED\"}\n",
    "label2id = {\"NON_VIOLATED\": 0, \"VIOLATED\": 1}\n",
    "model = LongformerForSequenceClassification.from_pretrained(\"allenai/longformer-base-4096\", num_labels=2, id2label=id2label, label2id=label2id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"../models/longformer_ecthr_model\",\n",
    "    learning_rate=4e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset= train_df2,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing global attention on CLS token...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='382' max='382' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [382/382 1:42:56, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.591395</td>\n",
       "      <td>0.796484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.647651</td>\n",
       "      <td>0.830709</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=382, training_loss=0.6243124357692859, metrics={'train_runtime': 6195.337, 'train_samples_per_second': 0.492, 'train_steps_per_second': 0.062, 'total_flos': 1001041856151552.0, 'train_loss': 0.6243124357692859, 'epoch': 2.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input ids are automatically padded from 441 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 264 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 172 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 417 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 428 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 153 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 439 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 284 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 226 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 260 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 276 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 442 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 390 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 392 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 225 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 281 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 279 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 496 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 270 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 326 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 275 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 505 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 307 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 341 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 239 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 355 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 453 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 468 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 248 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 292 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 340 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 223 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 366 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 459 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 448 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 320 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 220 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 318 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 285 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 473 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 330 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 345 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 476 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 221 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 461 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 402 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 394 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 332 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 470 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 407 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 393 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 403 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 344 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 357 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 398 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 404 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 395 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 346 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 429 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 430 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 400 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 416 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 301 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 269 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 329 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 424 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 451 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 444 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 258 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 455 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 478 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 389 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 432 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 310 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 506 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 313 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 277 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 484 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 479 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 457 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 399 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 507 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 291 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 195 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 384 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 288 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 232 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 251 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 272 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 354 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 348 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 510 to 512 to be a multiple of `config.attention_window`: 512\n"
     ]
    }
   ],
   "source": [
    "task_evaluator = evaluator(\"text-classification\")\n",
    "results_dict = {}\n",
    "for metric in [\"accuracy\", \"precision\", \"recall\", \"f1\"]:\n",
    "    results = task_evaluator.compute(\n",
    "        model_or_pipeline=\"../models/longformer_ecthr_model/checkpoint-382\",\n",
    "        data=test_dataset,\n",
    "        metric=metric,\n",
    "        tokenizer=tokenizer,\n",
    "        strategy=\"simple\",\n",
    "        random_state=0,\n",
    "        input_column='text',\n",
    "        label_column='labels',\n",
    "        label_mapping={\"NON_VIOLATED\": 0.0, \"VIOLATED\": 1.0},\n",
    "    )\n",
    "    metric_name, value = list(results.items())[0]\n",
    "    results_dict[metric_name] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.753,\n",
       " 'precision': 0.9439655172413793,\n",
       " 'recall': 0.7595375722543353,\n",
       " 'f1': 0.8417680973734785}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18291"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_dataset[0][\"input_ids\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "msc_thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
